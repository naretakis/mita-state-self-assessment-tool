# MITA 4.0 Maturity Model Reference Document

> Extracted from MGB Review_MITA 4 Maturity Model.docx

---

MITA Approval Request Form

Workgroup

SS-A and NextGen Workgroup

Date:

1/12/2026

Review Requested by:

1/23/2026

Type of Approval Requested

Artifact approval: MITA 4.0 Maturity Model Document

Detailed Description of Artifact

The Medicaid Information Technology Architecture (MITA) 4.0 Maturity Model reference document was developed to formalize the framework for the MITA 4.0 Maturity Model (MMM), present its maturity criteria, and gather feedback from the following:

•MITA NextGen Workgroups

•MITA Governance Board

•CMS

•MITA 4.0 Pilot Participants

Formal Ask

We are asking MGB members review, provide feedback If needed, and to recommend whether to approve this artifact to move forward to piloting.

Supporting Document

Please provide feedback and decision through survey: https://forms.office.com/g/R6HbbzGkX1

MGB Decision (Note whether approved, delayed requesting clarifying information or disapproved)

<Will be collected in survey linked to above>

MITA Maturity Model

Version 0.1 | August 8, 2025

Version History

Version

Date

Author

Notes

0.1

Z. Rioux

S. Lucas

Initial Release for pilot approval

Table of Contents

1Document Overview1

1.1Document Purpose1

1.2Audience1

2Model Overview1

2.1Maturity Model Purpose1

2.2Maturity Model Goals1

2.3Maturity Model Methodology and Guiding Principles2

2.4Maturity Model Levels and Definitions4

2.5Using the MITA Maturity Model in the MITA 4.0 Framework4

3Maturity Profile: A Standardized View of Maturity Assessment Results6

4Pilot Approach for Determining MITA 4.0 Maturity Assessment6

5Maturity Criteria7

5.1Outcomes (optional)7

5.2Roles (Optional)8

5.3Business Architecture10

5.4Information and Data13

5.5Technology18

6Using Personas To Describe Maturity Levels37

Appendix A Maturity Profile Standard Template39

List of Figures

Figure 1. MITA 4.0 Capability Reference Model Structure5

Figure 2. MITA 4.0 Maturity Scale with Personas38

Figure 3. Maturity Profile Standard Template39

List of Tables

Table 1. Outcomes (Optional) Maturity Criteria7

Table 2. Roles (Optional) Maturity Criteria8

Table 4. Business Architecture Maturity Criteria10

Table 5. Information and Data Architecture Maturity Criteria13

Table 6. Technical Architecture Maturity Criteria: Definitions19

Table 7. Technical Architecture Maturity Criteria: Infrastructure Domain20

Table 8. Technical Architecture Maturity Criteria: Integration23

Table 9. Technical Architecture Maturity Criteria: Platform Services26

Table 10. Technical Architecture Maturity Criteria: Application Architecture28

Table 11. Technical Architecture Maturity Criteria: Security and Identity31

Table 12. Technical Architecture Maturity Criteria: Operations and Maintenance33

Table 13. Technical Architecture Maturity Criteria: Development and Release35

Document Overview

Document Purpose

The Medicaid Information Technology Architecture (MITA) 4.0 Maturity Model reference document was developed to formalize the framework for the MITA 4.0 Maturity Model (MMM), present its maturity criteria, and gather feedback from stakeholders.

Audience

The primary audience for this document includes but is not limited to:

MITA 4.0 NextGen Members

MITA 4.0 State Self-Assessment (SS-A) Workgroup Members

MITA Governance Board

Centers for Medicare & Medicaid Services (CMS)

Model Overview

Maturity Model Purpose

The MITA Maturity Model provides State Medicaid Agencies (SMA) with a framework to systematically assess, benchmark, and improve processes, capabilities, architecture, and performance. The MMM offers SMAs a clear path to assess maturity, target specific areas for improvement, and achieve greater efficiency and effectiveness in delivery of Medicaid Program services.

Maturity Model Goals

The MITA 4.0 Maturity Model Workstream has established the following MMM goals:

Align with Strategic Objectives: Help ensure state capabilities directly support SMA goals.

Drive Standardization, Efficiency, and Consistency: Standardize and streamline processes across a SMA.

Enable Effective Transformation: Enable transformative and higher-quality outcomes with greater effectiveness and performance.

Optimize Resource Use and Integrate with MES Operations: Maximize impact while minimizing waste.

Strengthen Risk Management: Increase the capability to identify and mitigate risks.

Enable Measurable Progress: Provide measures to track improvement over time.

Maturity Model Methodology and Guiding Principles

To create the MITA 4.0 Maturity Model, the MITA Governance Board initiated a MITA 4.0 Maturity Model workstream tasked with the completion of the following deliverables:

Develop a MITA 4.0 Maturity Model.

Draft a MITA 4.0 Maturity Model Reference Guide that provides an overview of the MITA 4.0 approach to develop the MMM.

Collaborate with other workgroups to develop maturity criteria for the MMM based on the MITA 4.0 approach to using the SS-A for assessing the maturity of a capability and how well the SMA performs that capability. Capabilities will be assessed based on the outcomes (O), roles (R), business processes (B), information (I), and technology (T) (ORBIT) that a SMA has defined and implemented to support that capability. Maturity criteria are organized using the ORBIT framework.

Collaborate with other workgroups to develop a high-level overview of the MITA 4.0 process to better understand how to conduct the MMM and the MITA 4.0 SS-A.

To complete these four deliverables, the MMM Workstream carried out the following activities:

Phase 1 Initiation (October-December 2024): Identification of team members and definition of the workgroup’s purpose, goals, and approach. This phase also included defining MITA maturity and socialization of the workgroup’s purpose, goals, and approach.

Phase 2 Fact-finding and Visioning (November-January 2025): Conduct fact-finding with the Medicaid Governance Board (MGB), MITA 4.0 workgroups, and SMAs to understand SMA feedback on MITA 3.0’s maturity model as well as the opportunities for improvement with MITA 4.0. Phase 2 included a detailed review of MITA 3.0’s definition of maturity as well as other MITA 4.0 workgroups’ plans for the MITA 4.0 Maturity Model. The Workstream researched more than 60 industry-recognized maturity models to help determine which models might be leveraged for MITA 4.0. This effort also encompassed visioning sessions and efforts by The MITRE Corporation (MITRE) team to develop a draft MMM Scale for use by the MMM workgroup.

Phase 3 Finalize MITA 4.0 Maturity Model (June-October 2025): Socialize the draft MMM Scale, gather feedback, and develop both the MMM levels and supporting criteria i with other MITA 4.0 workgroups. The draft of this document was part of Phase 3.

Phase 4 Pilot MMM and Supporting Documentation (November 2025-March 2026): Socialize the MMM and supporting documentation with SMAs to gather their feedback and understand how the MMM aligns with the SMAs’ MITA 4.0 goals.

During Phase 2, the MMM Workstream consulted SMA users of the MITA 3.0 Maturity Models were consulted to understand their desired changes for the MITA 4.0 Maturity Model. Based on SMA responses, findings from review of other industry-recognized maturity models, and feedback from other MITA 4.0 workgroups, the MMM Workstream selected The Open Group Architecture Framework (TOGAF) as the baseline maturity model for the MITA 4.0 Maturity Model. The team drew from TOGAF’s maturity model to best align its guidance to best serve SMA .

The resulting MITA 4.0 Maturity Model retained or incorporated the following details:

Include five levels of maturity.

Add key words or descriptors to each maturity level to help users further understand the SMA’s maturity progression.

Allow for a “non-applicable” assessment rating that does not factor into maturity ratings.

Assess maturity for the individual MITA capability area.

Allow SMAs to average maturity scores and not tie the maturity rating to the lowest level of maturity. By averaging maturity scores, SMAs can demonstrate incremental progress across capability areas.

Enable assessment of levels beginning at the lowest possible state (non-compliance) and ending at the highest possible state (innovation and pioneering).

Present maturity criteria for each maturity level.

Ensure that maturity criteria address documentation, processes, technology, and/or systems that support:

Level One – a commitment to perform

Level Two – an ability to perform

Level Three – the activities performed

Level Four – the ability to measure and analyze

Level Five – the ability to implement, improve, and innovate

Maturity criteria should be part of the SS-A process to:

Assess current maturity levels

Identify gaps and desired to-be states

Populate the Outcomes-Based Planning process template

Inform Medicaid Enterprise Systems (MES) initiatives and the supporting Advance Planning Documents

Maturity Model Levels and Definitions

Maturity Model Levels and Definitions

Level 1 (Initial): SMA seeks to adopt enterprise-wide planning and architectural frameworks to improve program delivery. Current processes are unstructured, reactive, and inconsistent.

Level 2 (Developing): SMA complies with federal regulations and guidance and has begun adopting MES industry-recognized planning and architectural frameworks. Although basic processes and systems exist, they are not fully standardized or documented. The SMA collects and reports state-specific and MES metrics as well as performance data.

Level 3 (Defined): SMA complies with federal regulations and guidance and has fully implemented MES industry-recognized planning and architectural frameworks. Processes, systems, and strategies are standardized, well-documented, and aligned across the organization. The SMA actively monitors and analyzes state-specific and MES metrics as well as performance data for improvements.

Level 4 (Managed): SMA maintains compliance, follows industry-recognized planning and architectural frameworks, and monitors MES performance to meet goals. Processes are fully operational, consistent, and well executed. The SMA actively monitors and analyzes state-specific and MES metrics as well as performance data for improvements. The organization is a thought-leader in the MES ecosystem and actively collaborates and shares approaches with other SMAs.

Level 5 (Optimized): The SMA employs advanced, data-driven strategies to manage MES planning and architecture to align predictive decision-making with the SMA’s long-term goals. Integrated processes, technologies, and data drive enterprise optimization. The SMA’s institutionalized innovation supports adaptability, scalability, and continuous improvement. The organization is nationally recognized and actively collaborates and shares solutions with other SMAs.

Not Applicable: Not applicable was added by request of SMAs during listening sessions in late 2024. SMAs noted that some maturity criteria are inapplicable to their business operations or MES.

Using the MITA Maturity Model in the MITA 4.0 Framework

Background on MITA 4.0 Capabilities and the Capability Reference Model

A capability is defined as an ability that a SMA possesses or seeks to develop to achieve its goals and meet its desired outcomes. It represents what the SMA can do but without attempting to explain how, why or where the SMA uses the capability. A capability may exist within the SMA today or may be required to pursue a new direction or reach a new desired outcome as shown in Figure 1.

In MITA 4.0, each capability consists of the following:

Outcomes – The definition of the desired outcomes that require the capability to be achieved.

Roles – The individual roles responsible for providing the capability.

Business Processes – The business processes performed to deliver the capability.

Information – The information and the data management capabilities needed to deliver the capability.

Technology – The technology used to automate the capability.

A capability reference model is an abstract framework that defines concepts used for grouping capabilities with a common meaning. The reference model establishes a shared definition of capability concepts that can cross organizational boundaries and helps to identify opportunities for sharing, leveraging, and reuse.

The MITA 4.0 Capability Reference Model facilitates identifying the essential capability concepts to support the Medicaid Program and achieve the goals and outcomes established for MITA 4.0. The MITA 4.0 Capability Reference Model consists of two levels:

Capability Domain – High-level capability to group common capabilities

Capability Area – Detailed capabilities that decompose the capability domain into sub-capabilities that SMAs can used to classify their capabilities. Each Capability Domain has one-to-many distinct capability areas.

Figure 1. MITA 4.0 Capability Reference Model Structure

The Capability Reference Model Document provides more detail about the framework for MITA 4.0 capabilities.

MITA 4.0 Capabilities and Maturity Assessment

MITA 4.0 focuses on the capability that a SMA possesses or seeks to develop and is agnostic of how well the SMA performs that capability. The purpose of the state self-assessment is to assess the maturity of the state’s capability and how well the SMA performs the capability based on the outcomes, roles, business processes, business information, and technology architectures that a SMA has defined and implemented to support that capability.

Maturity Profile: A Standardized View of Maturity Assessment Results

The SMA should document the results of all maturity assessments in the Maturity Profile. This documentation present a concise, standardized view of the state’s self-assessment. CMS requests that SMAs document their results in a standard format to support CMS’s national view of maturity across all states and territories. To that end, CMS provides the Maturity Profile template in spreadsheet format for documenting maturity assessment results. SMAs can save the spreadsheet template as a .csv file for uploading to the MES Hub (MESH). Appendix A presents a sample Maturity Profile for submitting standardized maturity assessment results. The spreadsheet template format will be available for download on the MITA 4.0 Guidance site.

SMAs can use the proof-of-concept SS-A Tool to produce a Maturity Profile that conforms to the CMS spreadsheet template and employs the requisite .csv file for uploading into MESH.

If the SMA uses another tool or method to collate maturity assessment results, the SMA should provide those results to CMS in a format conforming to the Maturity Profile format via a .csv file.

Proposed delivery of MITA 4.0 SS-A Documentation to CMS:

SMAs should upload their maturity assessment results in a format that conforms to the standardized view provided in the Maturity Profile template. A .csv file should be generated to import into MESH. Once the .csv file is imported, a state should notify their State Officer that the file is available on MESH.

Pilot Approach for Determining MITA 4.0 Maturity Assessment

The maturity criteria for Outcomes, Roles, and the Business, Information, and Data architectures will be incorporated into the proof-of-concept SS-A Tool. The MMM Workstream will collect assessment scores using the proof-of-concept SS-A Tool for automated scoring. Pilot SMAs will use the tool to produce an output that conforms to CMS requirements for a standardized view.

Maturity Criteria

MITA 4.0 maturity criteria are organized using the ORBIT framing.

Outcomes (optional)

The following Outcomes maturity criteria are optional for adoption and use by the states. These criteria provide guidance and examples of the qualities and practices currently employed in an outcomes-oriented organization.

Table 1. Outcomes (Optional) Maturity Criteria

Aspect

Level 1: Initial

Level 2:Developing

Level 3:Defined

Level 4:Managed

Level 5:Optimized

Culture & Mindset

Organizational norms, expectations, and behaviors related to outcome development.

No culture or expectation of outcomes for projects (at any phase of the project).

Recognition begins to take hold of the importance of outcomes during planning.

Leadership sets expectations; outcome development during planning becomes a norm.

Outcome development is formalized and institutionally expected.

Outcome thinking is fully embedded at all levels of organization—without direction by or requirement from leadership.

Capability

The internal capacity to develop, oversee, and apply outcomes without reliance on vendors.

No internal capabilities to identify or draft outcomes.

Organization understands at a high-level what elements are needed to identify and draft outcomes, but vendor leads development of outcomes.

Sufficient experience in identifying and drafting outcomes internally; organization may lead or closely oversee vendors to draft outcomes.

Full in-house capability to independently produce outcomes.

Deep in-house expertise with fully self-sufficient teams across the organization.

Organization may also support sister organizations and agencies in developing outcomes.

Quality & Consistency

The clarity, structure, and strategic alignment of outcome statements.

Outcomes are ambiguous, inconsistent, and disconnected from goals.

No consistent approach or methodology to developing outcomes.

Outcomes may be aligned with priorities but are not “smart” or actionable.

Defined approach or methodology to developing outcomes.

Outcomes capture the capability sought and benefit derived from a given project, but the quality of the outcome statements vary.

Well-established and consistently applied approach to developing outcomes.

Outcomes are clear, actionable, reusable, and strategically aligned.

Well-established and consistently applied approach to developing outcomes.

Outcome statements are of consistently high quality, recognizable, and standardized.

Alignment to Goals & Priorities

The extent to which outcomes are traceable to broader goals, priorities, and mission.

Outcomes are not aligned to goals or priorities.

Emerging awareness of need to align outcomes with goals and priorities but gaps remain.

Outcomes are intentionally aligned to strategic goals and priorities.

Systematic alignment of outcomes to goals and priorities.

Outcomes inform broader strategies.

Outcomes reflect and reinforce strategic direction seamlessly.

Use of Metrics

How outcomes are tracked, and metrics are used to measure progress and guide improvement.

No recognition of the need to measure progress toward achieving outcomes or commitment to continuous improvement.

Interest in measuring progress toward achieving outcomes.

Metrics are not well-aligned with outcomes.

Metrics are generally appropriate and regularly referenced to determine progress in achieving outcomes.

Some engagement in continuous improvement cycles.

Metrics are well-developed.

Staff regularly use metrics to evaluate performance and engage in well-established continuous improvement cycles.

Performance monitoring is deeply embedded in organization’s culture.

Metrics and outcomes support continuous improvement, learning, and decision-making.

Reusability & Integration

Whether outcomes are reused across project documents, Advance Planning Documents (APD), systems, and reporting.

No reuse.

Outcomes are informal or nonexistent.

Reuse is isolated to APDs; no reuse across other project, planning, or leadership documents.

Reuse is focused on APDs and some project-related documents.

Inconsistent reuse across leadership documents, and external communications.

Reuse is consistent and intentional across APDs, project documents, and leadership and external communications.

Seamless reuse across systems and communications, which improves coherence and efficiency.

Roles (Optional)

The Roles maturity criteria in Table 2 are optional for the states. These criteria are examples of the current qualities and practices that a SMA and its leaders possess or should consider adopting.

Table 2. Roles (Optional) Maturity Criteria

Aspect

Level 1: Initial

Level 2: Developing

Level 3: Defined

Level 4: Managed

Level 5: Optimized

Technology Resources

Partial list of technical resources exists; capabilities are not fully understood across the SMA.

SMA maintains a comprehensive list of all technical resources and their capabilities.

SMA maps technical resources directly to business processes.

Technical resources are actively shared and optimized across business units for seamless execution.

Technical resources are accessible to agencies, states, and federal partners and support continuous improvement through data-driven insights and collaborative efforts; drives technical reforms across the broader health and human services network.

Organizational Goals Alignment

Business units operate in silos without clear understanding of enterprise-wide Medicaid goals.

Organizational goals are defined but not fully communicated across all levels.

Business units align their roles and processes to support broader Medicaid goals.

All roles (internal and external) collaborate to drive enterprise-wide Medicaid program goals, with regular reassessments for alignment.

SMA leadership collaborates at the national level to set adaptable, data-driven goals and performance outcomes for the Medicaid program; focus is on improving healthcare and social services delivery and beneficiary well-being nationwide.

Governance & Standardization

Limited or no formal governance to guide roles or standardize processes.

Some governance exists, but role clarity and process standardization are inconsistent.

Formal governance structures support role clarity, with standard operating procedures (SOP).

Enterprise governance bodies actively coordinate roles, processes, and data for continuous improvement and standardization.

SMA leads data-driven solutions at a national level and collaborates with other SMAs to develop shared business and technical standards that are disseminated nationally to strengthen overall Medicaid service delivery.

Communication (Internal / External)

Minimal communication between leadership, business, and technical units.

Defined communication channels exist but are not used consistently.

Regular cross-unit communication occurs, including with external partners like Managed Care Organizations (MCO) and vendors.

Transparent, enterprise-wide communication ensures all roles understand priorities, responsibilities, and impacts.

Integrates communication across all business areas to align long-term Medicaid goals and performance outcomes.

Fosters cooperation with other SMAs, CMS, and external partners nationally.

The Agency is recognized for its leadership in sharing solutions, provider enrollment process needs, and best practices.

Culture & Leadership

Leadership lacks visibility into workloads and resource capacity; roles are reactive.

Leadership begins assessing role capacity but lacks full enterprise insight.

Leadership proactively manages workloads and roles based on enterprise needs and priorities.

Leadership fosters a culture of collaboration and continuous learning across internal and external roles.

Leadership supports internal change agents to align culture, strategy, and resources with long-term Medicaid goals.

Resourcing Capacity (Staffing, Training, Recruitment

SMA struggles to identify gaps in role capacity; staffing is reactive.

Some processes exist for role capacity management (staffing, training, recruitment).

SMA systematically identifies, recruits, and trains staff.

SMA maintains a dynamic capacity model to forecast and meet enterprise role needs, including succession planning and continuous role development.

SMA uses comprehensive planning and advanced analytics to align roles and skills with program objectives, enabling rapid response to required changes while maintaining high service delivery standards.

Business Architecture

Table 3 presents the Business Architecture maturity criteria.

Table 3. Business Architecture Maturity Criteria

Aspect

Level 1: Initial

Level 2: Developing

Level 3: Defined

Level 4:Managed

Level 5:Optimized

Business Capability

The capability has been identified, and the use of the capability has been documented.

The SMA has begun the documentation of the capability roles, supporting processes, technology, and information.

The capability complies with state and federal requirements.

The SMA has fully documented and defined the capability roles, supporting processes, technology, and information.

The capability complies with state and federal requirements and conforms to MES industry-recognized architectural frameworks.

The capability is standardized and aligned at the enterprise level.

The SMA manages the capability for performance and compliance.

The SMA monitors the metrics and measures of the capability.

The SMA uses performance monitoring to make improvements to the capability.

The SMA is a thought leader for the capability and shares capability information with other SMAs.

Business Process

The business process(es) that support a capability have been identified and defined.

The business process(es) that support a capability have been documented in SOPs with steps and roles responsible for each activity or decision.

All associated staff members supporting the capability are following the same process steps in the same order for a standardized, repeatable approach to the work.

All processes are easily accessible in a Business Process Catalog.

The business process(es) that support a capability have associated performance metrics (e.g., timeliness, quality, and efficiency) and are regularly tracked and reported to leadership to understand bottlenecks in the process.

A few business process steps are automated to streamline overall turnaround time without sacrificing quality.

The business process(es) that support a capability are regularly assessed using the performance metrics to identify process improvements to increase SMA effectiveness and efficiency.

All business process steps are automated as much as possible to streamline overall turnaround time without sacrificing quality.

Business Process Model

Business process models are not fully developed for the supporting capability.

Business Process Models are developed for all business processes supporting the capability and for staff reference.

The business process model(s) that support a capability follow Business Process Modeling Notation (BPMN) 2.0 or other organization defined standard for business process documentation to ensure industry best practices are applied to the documentation.

The business process model(s) that support a capability are developed in a standardized tool for version control and historical tracking of changes and their impact on metrics.

Business processes are managed in an enterprise catalog.

The business process model(s) are monitored and reviewed for improvement and updates for improvements.

Role Management

The Role(s) that support the process are identified as part of the process documentation or process model.

Strategic Planning

The SMA’s strategic plan is considered in the architectural and capability planning.

Aligned where possible.

The SMA’s strategic plan and capability planning are aligned through outcomes.

The SMA’s strategic plan has identified capabilities as a dependency to achieve outcomes.

Capabilities and processes are identified and analyzed as part enterprise risk and issue management.

Capabilities and processes are identified and analyzed as part of Enterprise Portfolio and Project Management.

The SMA’s Enterprise Roadmap (Projects / Initiatives) has identified capabilities and processes that support the roadmap items.

Capabilities and processes are identified and analyzed for intake of new business to support funding requests for an APD or for state funding for project approval.

Capabilities and processes are identified and analyzed in Enterprise Governance for decision-making.

Enterprise Architecture

SMA recognizes the need for enterprise-wide planning and architectural frameworks; however, current Enterprise Architecture (EA) practices are unstructured, reactive, and inconsistent.

SMA complies with federal guidance and has begun adopting MES-recognized EA frameworks.

EA processes exist but are not standardized or fully documented.

SMA has fully implemented MES-recognized EA frameworks.

EA processes are standardized, documented, and aligned across business, information, and technology.

SMA uses EA to monitor and manage MES performance.

EA is integrated into strategic planning and execution.

SMA collaborates with other SMAs to share EA practices.

SMA uses advanced, data-driven EA strategies for proactive decision-making.

EA drives enterprise optimization and continuous improvement.

SMA is nationally recognized for EA leadership.

Policy Management

Policies relevant to business capabilities are informally identified or inconsistently documented.

There is little to no mapping of policies to business processes or systems.

Some policies are documented, and the organization is starting to map policies to business processes and systems.

Compliance is mostly reactive; regular reviews are not established.

All business policies relevant to the capability (e.g., “Pharmacy Management”) are identified, documented, and mapped to business processes and supporting systems.

Policies are harmonized with state and federal requirements, and their alignment is regularly reviewed to ensure compliance and support for enterprise objectives.

Policy alignment is institutionalized and integrated into enterprise governance.

Policies are systematically reviewed, updated, and optimized based on performance data, regulatory changes, and strategic objectives.

Policy alignment is a proactive, continuous process that drives organizational agility, compliance, and innovation.

The organization demonstrates thought leadership and collaborates externally to share best practices.

Information and Data

Table 4 presents the Information and Data Architecture maturity criteria.

Table 4. Information and Data Architecture Maturity Criteria

Aspect

Level 1: Initial

Level 2: Developing

Level 3: Defined

Level 4: Managed

Level 5: Optimized

Data Storage & WarehousingDoes the SMA manage the storage and warehousing of the information assets that support this capability?

Storage locations for the information assets that support this capability have been identified and documented.

Basic storage and warehousing requirements for these information assets have been defined.

Storage and warehousing procedures have been defined and documented.

Roles and responsibilities for managing the storage and warehousing of these information assets have been defined.

Storage and warehousing standards and conventions (such as structure, naming, or access rules) have been defined for the information assets.

Storage and warehousing roles for this capability have been assigned to individuals, and they are actively performing their responsibilities.

Storage and warehousing standards and conventions have been adopted and are applied to the information assets that support this capability.

Storage and warehousing performance measures have been established for these information assets.

Storage and warehousing performance and maturity assessments are conducted regularly for the information assets that support this capability.

Root cause analysis is performed and documented for issues affecting the storage or warehousing of the information assets.

Storage and warehousing improvement activities needed to increase performance and maturity are regularly identified and carried out for the information assets that support this capability.

Data Architecture & ModelingDoes the SMA manage the data architecture and models for the information assets that support this capability?

The data architecture components needed to support this capability have been identified and documented.

Basic data modeling requirements for the information assets that support this capability have been defined.

Data architecture and data modeling procedures for how each information asset is structured and represented have been defined and documented.

Roles and responsibilities for managing the data architecture and data models for these information assets have been defined.

Data architecture standards, modeling conventions, and reference structures needed to support these information assets have been defined.

Data architecture and data modeling roles for this capability have been assigned to individuals, and those individuals are actively performing their responsibilities.

Data architecture standards, modeling conventions, and reference structures have been adopted and applied to the information assets that support this capability.

Data architecture and modeling performance measures have been established for these information assets.

Data architecture and modeling performance assessments are conducted regularly for the information assets that support this capability.

Root cause analysis is performed and documented for issues affecting the data architecture or models of the information assets.

Data architecture and modeling improvement activities needed to increase performance and maturity are regularly identified and carried out for the information assets that support this capability.

Document & Content ManagementDoes the SMA manage the document and content information assets that support this capability?

The document and content information assets needed to support this capability have been identified and documented.

Basic document and content management requirements for these information assets have been defined.

Procedures for how each document and content asset is created, stored, updated, and archived have been defined and documented.

Roles and responsibilities for managing document and content information assets for this capability have been defined.

Document and content management roles for this capability have been assigned to individuals, and those individuals are actively performing their responsibilities.

Document and content management standards, conventions, and retention rules have been adopted and applied to the information assets that support this capability.

Document and content management performance measures have been established for these information assets.

Document and content management performance assessments and maturity assessments are conducted regularly for the information assets that support this capability.

Root cause analysis is performed and documented for issues affecting the document and content information assets.

Document and content management improvement activities needed to increase performance and maturity are regularly identified and carried out for the information assets that support this capability.

Data Governance

Does the SMA govern the information assets that support this capability?

Information assets needed to support the capability have been identified and documented.

Data governance requirements for the information assets for this capability have been defined.

Definition for how each information asset is used has been defined and documented.

Data governance roles and responsibilities needed to support the information assets for this capability have been defined.

Data policies, standards, reference models and specifications needed to support the information assets for this capability have been defined.

Data issues that impact the information assets for this capability have been defined and documented.

Data governance roles needed to support this capability have been assigned to individuals and they are actively performing their assigned responsibilities.

Data governance information is stored centrally and accessible to key stakeholders that need access to it.

Data policies, standards, reference models, and specifications have been adopted and aligned to the information assets for this capability.

Data governance performance measures and standards have been established to support the information assets for this capability.

Data governance performance assessments are performed regularly on the information assets for this capability.

Enterprise data governance capability maturity assessments are performed and aligned to the information assets for this capability.

Root cause analysis for the data issues that impact the information assets for the capability have been performed and documented.

Data governance activities needed to improve the performance and maturity of data governance of the information assets for this capability are regularly identified and performed.

Data Privacy & Security

Does the SMA manage the privacy and security of the information assets that support this capability?

Privacy and security requirements for the information assets that support this capability have been identified and documented.

Basic protections (such as access restrictions or password controls) for these information assets have been defined.

Procedures for how each information asset is protected, accessed, transmitted, and stored have been defined and documented.

Privacy and security roles and responsibilities for managing these information assets have been defined.

Privacy and security policies, standards, and controls (such as encryption requirements, data handling rules, and access control standards) have been defined.

Privacy and security roles for this capability have been assigned to individuals, and those individuals are actively performing their responsibilities.

Privacy and security policies, standards, and controls have been adopted and applied to the information assets that support this capability.

Privacy and security performance measures have been established for these information assets.

Privacy and security performance assessments and maturity assessments are performed regularly for the information assets that support this capability.

Root cause analysis is performed and documented for privacy and security incidents or issues affecting these information assets.

Privacy and security improvement activities needed to increase performance and maturity are regularly identified and carried out for the information assets that support this capability.

Data Quality

Does the SMA manage the quality of the information assets that support this capability?

The data quality requirements for the information assets that support this capability have been identified and documented.

Basic expectations for accuracy, completeness, consistency, and timeliness have been defined.

Procedures for how data quality is monitored, measured, and corrected for each information asset have been defined and documented.

Roles and responsibilities for managing data quality for these information assets have been defined.

Data quality standards, business rules, and validation requirements have been established.

Data quality roles for this capability have been assigned to individuals, and those individuals are actively performing their responsibilities.

Data quality standards and business rules have been adopted and are consistently applied to the information assets that support this capability.

Data quality performance measures and thresholds have been established for these information assets.

Data quality performance assessments and maturity assessments are regularly conducted for the information assets that support this capability.

Root cause analysis is performed and documented for data quality issues or defects affecting these information assets.

Data quality improvement activities needed to increase performance and maturity are regularly identified, prioritized, and carried out for the information assets that support this capability.

Data Integration & Interoperability

Does the SMA integrate the information assets that support this capability into the enterprise and maintain interoperability?

Data exchanges needed for this capability have been identified.

Structural / Transaction standards needed to support this capability have been defined and documented.

Mapping specifications needed to support the integration of each structure / transaction have been identified.

Structure / Transaction data standards have been adopted and implemented.

Transaction data is actively received and integrated into the enterprise and is accessible to key stakeholders that need access to it.

Data integration & interoperability performance measures and standards have been established to support this capability.

Data Integration & Interoperability performance assessments are performed regularly on the information assets for this capability.

Enterprise Data Integration & Interoperability capability maturity assessments are performed and aligned to the information assets for this capability.

The SMA continuously identifies and implements improvement initiatives aimed at enhancing integration performance and interoperability maturity.

Practices such as automation of data exchanges, adoption of emerging standards, and reuse of integration components are regularly pursued.

Progress and impact of improvement activities are measured through key performance indicators (KPI), and lessons learned are integrated into future initiatives to sustain growth in integration maturity.

Master Data Management

Does the SMA manage the master data that is needed to support this capability?

Master data entities needed to support the capability have been identified.

Master data management requirement to support this capability have been defined.

The data sources that impact the master data for this capability have been identified.

Master data rules / logic that support this capability have been defined and documented.

Master data is accessible to the key stakeholders who need it to support this capability.

Master data performance measures and standards have been established to support this capability.

Master data management performance assessments are performed regularly on the information assets for this capability.

Enterprise master data management capability maturity assessments are performed and aligned to the information assets for this capability.

Activities needed to improve the performance and maturity of master data management of the information assets for this capability are regularly identified and performed.

Reference Data Management

Does the SMA manage the reference data that is needed to support this capability?

Reference data needed for this capability have been identified.

Reference data requirements for this capability have been defined.

Reference data standards needed to support this capability have been defined and documented.

Crosswalks needed to support this capability have been identified.

Crosswalks for the reference data needed to support this capability have been documented.

Reference data needed to support the capability is stored centrally and accessible to key stakeholders that need access to it.

Reference data performance measures and standards have been established to support this capability.

Regular assessments of reference data quality and management practices are conducted, including audits and validation activities to identify gaps and improvement opportunities.

Root cause analyses are performed and documented for issues impacting reference data, with corrective actions implemented.

Monitoring tools, audit logs, and reporting mechanisms are maintained to ensure ongoing compliance and quality.

The SMA continuously identifies and executes improvement activities aimed at enhancing reference data quality, consistency, and management maturity.

Practices such as automation, standardization, and integration of reference data processes are adopted to increase efficiency.

The organization actively participates in or contributes to industry standards and best practices for reference data management.

The impact of improvement initiatives is measured through KPIs, and lessons learned are incorporated to sustain ongoing maturation of reference data management capabilities.

Business Intelligence

Does the SMA provide business intelligence (BI) to support his capability?

Measures and Reports needed for this capability have been identified.

Business Intelligence requirements for this capability have been defined.

Measure and report specifications needed for this capability have been defined and documented.

Reports needed for this capability can be traced to the specific data elements that are used in the report.

Measures and Reports needed for this capability are stored centrally and accessible to key stakeholders that need access to it.

Business Intelligence performance measures and standards have been established to support this capability.

Regular assessments of BI performance and data quality are conducted, including validation, user feedback, and usage analysis.

Root cause analysis is performed for issues such as incorrect reports or data discrepancies, with corrective actions documented and tracked.

Monitoring dashboards and audit logs are maintained to oversee BI system performance, security, and compliance.

The SMA continuously identifies and implements improvement activities to enhance BI capabilities, data quality, and user experience.

Practices such as automation, advanced analytics, and self-service BI are adopted to increase agility and insight generation.

The organization actively explores new BI technologies, data visualization techniques, and analytics methods to increase maturity.

The impact of BI improvements is measured through KPIs such as decision-making speed, data accuracy, and stakeholder satisfaction, with lessons learned incorporated into ongoing initiatives.

Metadata Management

Does the SMA manage the metadata for the information assets that support this capability?

Metadata requirements for the information assets for this capability have been defined.

Metadata standards and expectations have been defined for the metadata needed to manage the information assets for the capability.

Metadata about the information assets for this capability have been identified and documented.

Metadata about the information assets for this capability are stored centrally and accessible to key stakeholders that need access to it.

Metadata performance measures and standards have been established to support the information assets for this capability.

Metadata performance assessments are performed regularly on the information assets for this capability.

Enterprise metadata capability maturity assessments are performed and aligned to the information assets for this capability.

Activities needed to improve the performance and maturity of metadata management of the information assets for this capability are regularly identified and performed.

Technology

Table 5 presents the Technical Architecture maturity criteria.

Table 5. Technical Architecture Maturity Criteria: Definitions

Level 1:Initial

Level 2:Developing

Level 3:Defined

Level 4: Managed

Level 5: Optimized

The State operates a monolithic, legacy MMIS with little modernization or capability to interact with other systems as evidenced by limited modularity, reuse, or interoperability.

Integrations occur through direct connections, using proprietary formats and batch file transfers.

Security practices are fragmented; compliance processes are manual with limited automation.

The State has begun modularizing core Medicaid functions (e.g., eligibility and claims) and piloting standards-based application programming interfaces (API) (e.g., National Information Exchange Model [NIEM] and Health Level 7 [HL7]).

Non-critical modules are piloted in Software as a Service (SaaS) environments; security policies exist but lack centralized enforcement.

Security policies are in place, but enforcement remains siloed.

The state is implementing modular components and piloting cloud solutions and standards, but it lacks overall enterprise integration.

Core Medicaid systems are modularized with standardized service contracts and interoperable APIs aligned with CMS and national standards. (e.g., Fast Healthcare Interoperability Resources [FHIR], HL7, and X12).

A hybrid cloud model supports scalability and modernization, with a cloud-first policy for new modules.

State runs a modular, standards-based Medicaid enterprise that aligns with CMS’s requirements for modularity and interoperability.

The Medicaid Enterprise is governed as a cohesive platform with real-time interoperability across modules, MCOs, Health Information Exchanges (HIE), and CMS.

Performance metrics are actively monitored and used for continuous improvement.

Most of the Medicaid Enterprise is hosted in the cloud and designed for flexibility and reliability.

States manage the Medicaid Enterprise as a cloud-based platform.

The enterprise is fully composable, enabling dynamic orchestration of services and seamless integration with broader human services (e.g., Supplemental Nutrition Assistance Program [SNAP], Transitional Assistance for Needy Families [TANF], and behavioral health).

The Medicaid Enterprise is cloud-native and serverless, ensuring resilience and scalability across the nation.

A zero-trust architecture is fully implemented; compliance is continuous and automated.

The state manages a next-generation, adaptive Medicaid enterprise.

Table 6 presents the Technical Architecture maturity criteria for Infrastructure Domain.

Table 6. Technical Architecture Maturity Criteria: Infrastructure Domain

Infrastructure Domain

Aspect

Level 1:Initial

Level 2: Developing

Level 3: Defined

Level 4: Managed

Level 5: Optimized

The infrastructure is legacy bound, primarily on-premises, with minimal virtualization or automation.

Networking is flat and insecure, storage is unmanaged, and resilience is reactive.

There is no formal governance, monitoring, or scaling capability.

The state begins adopting virtualization and piloting cloud services for non-critical workloads.

Basic security and backup policies are introduced.

Networking is segmented, and Disaster Recovery (DR) plans exist but are untested.

Governance roles are emerging, but practices are inconsistent.

A hybrid cloud model is in place with standardized provisioning and container adoption.

Storage is tiered and policy driven.

Secure, segmented networking supports external partners.

DR is tested, and auto-scaling is implemented for some workloads.

Technical debt is tracked, and observability is established.

The infrastructure is governed by a cloud-first policy, with container orchestration, Infrastructure-as-Code (IAC), and integrated monitoring.

Storage and networking are optimized for performance, cost, and compliance.

DR and scaling are automated and aligned with business KPIs.

Zero-trust principles are emerging.

The infrastructure is fully cloud native, composable, and adaptive.

Serverless and edge computing are used where appropriate.

Artificial Intelligence (AI) / Machine Learning (ML) supports predictive scaling, anomaly detection, and self-healing.

Zero-trust networking and continuous compliance are fully implemented.

Governance is dynamic and data driven.

Infrastructure Detail

Compute and Hosting

All workloads run on legacy on-premises servers.

No virtualization or cloud usage.

No centralized inventory of compute assets.

Some virtualization is introduced.

Cloud pilots are used for non-critical workloads.

Compute provisioning is manual and ad hoc.

A hybrid cloud model is adopted.

Virtual machines (VM) and containers are provisioned using standardized templates.

Compute assets are inventoried and monitored.

Cloud-first policy is enforced.

Container orchestration (e.g., Kubernetes) and IAC are used for provisioning.

Compute resources are monitored for performance and cost.

Fully cloud-native and serverless where appropriate.

Compute provisioning is dynamic and policy driven.

AI/ML supports predictive scaling and self-healing.

Storage

Local storage only. Manual backups.

No encryption or retention policies.

Storage growth is unmanaged.

Basic Storage Area Network (SAN) / Network Attached Storage (NAS) is introduced.

Some automated backups.

Retention policies are drafted but not enforced.

Tiered storage is implemented.

Cloud backups and encryption are enforced.

Immutable backups are used for critical data.

Storage lifecycle is automated.

Cost optimization and anomaly detection are in place.

Storage is aligned with data classification policies.

AI/ML-driven tiering and self-healing storage.

Continuous compliance with Health Insurance Portability and Accountability Act (HIPAA), National Institute of Standards and Technology (NIST), and state policies.

Storage adapts to usage and risk patterns.

Networking and Connectivity

Flat network with no segmentation.

No secure external access.

No formal documentation or monitoring.

Virtual Private Networks (VPN) and firewalls are introduced.

Some secure partner access.

Basic network monitoring is in place.

Network is segmented using Virtual Local Area Networks (VLAN) or security zones.

Redundant connectivity is established.

Real-time dashboards monitor traffic.

Software-Defined Wide Area Network (SD-WAN) or cloud-native networking is adopted.

Zero-trust principles are emerging.

Network traffic is monitored and alerts are automated.

Fully zero-trust architecture.

Dynamic routing and posture assessment.

AI-driven analytics continuously optimize network performance and security.

Resilience and Scaling

No DR plan.

Failover is manual.

No scaling capabilities.

Outages are handled reactively.

DR plan exists but is untested.

Manual load balancing is used.

Basic scripts support limited scaling.

DR is tested annually.

Auto-scaling is implemented for some workloads.

Resilience metrics (e.g., Recovery Time Objective [RTO] / Recovery Point Objective [RPO]) are tracked.

DR is automated and tested regularly.

Elastic scaling is tied to business KPIs.

Resilience is monitored in real time

Predictive resilience using telemetry and AI.

Seamless DR with no user impact.

Scaling is adaptive and policy driven.

Infrastructure Assessment / Evidence (E)

Compute and Hosting

Q: Are most workloads hosted on physical servers? (Yes/No)Q: Is there a centralized inventory of compute assets? (Yes/No)

E: Server inventory spreadsheetE: Screenshots of legacy on-premise systems

Q: Are non-critical workloads piloted in the cloud? (Yes/No)

Q: Are virtualization tools in use? (Yes/No)

E: Cloud pilot documentation

E: Virtualization deployment plan

Q: Is there a hybrid cloud model in place? (Yes/No)

Q: Are provisioning processes standardized? (Yes/No)

E: Hybrid cloud architecture diagram

E: VM provisioning templates

Q: Is IaC integrated into Continuous Integration (CI) / Continuous Delivery (CD) pipelines? (Yes/No)

Q: Are containers orchestrated using a platform like Kubernetes? (Yes/No)

E: IaC scripts in version control

E: Deployment manifests

Q: Are compute resources dynamically provisioned based on usage? (Yes/No)

Q: Is serverless computing used in production? (Yes/No)

E: Serverless architecture diagrams

E: Scaling dashboards using AI/ML

Storage

Q: Are backups performed manually? (Yes/No)

Q: Are storage growth and retention unmanaged? (Yes/No)

E: Backup logs (manual)

E: Storage device inventory

Q: Are retention policies defined but not enforced? (Yes/No)

Q: Are backups partially automated? (Yes/No)

E: Draft retention policy

E: SAN/NAS configuration screenshots

Q: Is encryption enforced for data at rest and in transit? (Yes/No)

Q: Are immutable backups used? (Yes/No)

E: Tiered storage policy

E: Cloud backup configuration

Q: Is storage lifecycle automated based on policy? (Yes/No)

Q: Are anomalies in storage usage detected automatically? (Yes/No)

E: Storage lifecycle automation scripts

E: Cost optimization reports

Q: Is storage self-healing and AI-optimized? (Yes/No)

Q: Is compliance continuously validated? (Yes/No)

E: Tiering engine logs

E: Compliance audit reports (HIPAA, NIST)

Networking and Connectivity

Q: Is the network flat and unsegmented? (Yes/No)

Q; Is there secure external access? (Yes/No)

E: Network topology diagram (flat)

Q: Are VPNs and firewalls in place? (Yes/No)

Q: Is partner access manually provisioned? (Yes/No)

E: Network topology diagram

E: Partner access documentation

Q: Is network segmentation enforced? (Yes/No)

Q: Are dashboards used to monitor traffic? (Yes/No)

E: VLAN / security zone documentation

E: Real-time network dashboards / screenshots

Q: Is zero-trust architecture emerging? (Yes/No)

Q: Is network traffic monitored in real time? (Yes/No)

E: SD-WAN Deployment Plan

E: Zero-trust documentation

Q: Is network posture continuously assessed and adjusted? (Yes/No)

Q: Is dynamic routing implemented? (Yes/No)

E: Network analytics

E: Dynamic routing documentation

Resilience and Scaling

Q: Is there a documented DR plan? (Yes/No)

Q: Are failovers handled manually? (Yes/No)

E: DR Plan (if any)

E: Failover logs

Q: Are DR plans tested regularly? (Yes/No)

Q: Is load balancing manual or scripted? (Yes/No)

E: Load balancing documentation

E: DR test schedule (Draft)

Q: Are resilience metrics tracked? (Yes/No)

Q: Is auto-scaling implemented for some workloads? (Yes/No)

E: Auto-scaling

E: RTO / RPO metrics

Q: Is DR automated and aligned with KPIs? (Yes/No)

Q: Is scaling tied to real-time demand? (Yes/No)

E: Elastic scaling policy

E: DR automation documentation

Q: Is resilience predictive and AI-driven? (Yes/No)

Q: Are outages mitigated before they occur? (Yes/No)

E: Analytics Dashboard

E: Failure mitigation documentation

Table 7 presents the Technical Architecture maturity criteria for Integration.

Table 7. Technical Architecture Maturity Criteria: Integration

Integration

Aspect

Level 1:Initial

Level 2:Developing

Level 3:Defined

Level 4:Managed

Level 5:Optimized

The interfaces are single purpose with minimal use of data standards and no documentation.

There is little to no oversight.

System messages are not exchanged or consolidated.

Partner integrations are lacking consistency.

Interface patterns and data standards are developed for reuse.

New and updated interfaces are subject to oversight and require documentation.

Systems exchange some messages, but exchanges lack standards.

Partner integration patterns are under development.

New interface designs are based on reuse and data standards.

Governance and documentation standards are in place.

Patterns for exchanging messages between systems are in place.

External partner integration follows standard processes and design.

Software frameworks are widely used and there is extensive interface reuse.

Data standards are applied to interfaces and interface specifications are published.

Messages for critical information are exchanged across systems using standard patterns and tools.

Eliminating variation of partner integration and automating endpoint management.

Single use interfaces are eliminated.

Interfaces follow patterns and implement data standards.

Specifications are published using API management tools.

Systems exchange messages using tools that trigger alerts when needed and track problems through to resolution.

Partner integration is standardized, and endpoints are continually monitored.

Integration Detail

API and Interface Management

Interfaces are point-to-point, single purpose, and inconsistently designed.

Data standards are rarely applied.

No governance or documentation exists.

Interface patterns and data standards are defined.

Some reuse is piloted.

Documentation and oversight are introduced for new interfaces.

New interfaces follow defined patterns and apply data standards (e.g., FHIR and X12).

Governance and documentation standards are enforced.

Interfaces are widely reused.

API specifications are published using API management tools.

Governance drives standardization and lifecycle management.

All interfaces follow standardized patterns and data standards.

API specifications are discoverable and versioned.

Interface behavior is monitored and dynamically adjusted based on usage and policy.

System Messaging

No messaging between systems.

Data is exchanged manually or via batch files.

Some messaging is introduced using non-standard formats.

Alerts are manually triggered.

Messaging patterns are standardized.

Messages trigger alerts and are logged.

Messaging is used for key business events.

Messaging is used for real-time, critical information exchange.

Alerts are traceable and integrated with monitoring tools.

Messaging is event-driven and proactive.

Messages trigger automated workflows and predictive alerts.

Messaging patterns are reused across the enterprise.

External Partner Integration

Partner integrations are ad hoc.

No standards or endpoint management practices exist.

Integration patterns and endpoint management processes are defined.

Some partners follow emerging standards.

New integrations follow standard patterns.

Endpoints are registered and managed.

Security and data-sharing agreements are documented.

Most partner integrations use standardized patterns.

Endpoint management is automated.

Integration health is monitored.

Partner integrations are consolidated and governed through shared services.

Endpoint management is integrated with Information Technology Service Management (ITSM).

Real-time monitoring and automated remediation are in place.

Integration Assessment / Evidence

API and Interface Management

Q: Are interfaces primarily point-to-point and single purpose? (Yes/No)

Q: Is interface documentation available? (Yes/No)

E: Interface inventory (if any)

E: Screenshots of point-to-point connections

Q: Are interface patterns defined? (Yes/No)

Q: Are new interfaces subject to review or documentation? (Yes/No)

E: Draft interface patterns

E: Sample API specifications using emerging standards

E: Governance meeting notes

Q: Are APIs designed using standardized patterns? (Yes/No)

Q: Are APIs versioned and documented? (Yes/No).

Q: Is interface governance enforced? (Yes/No)

E: API design templates

E: Interface governance policy

E: Versioned API documentation

Q: Are APIs published and discoverable via a management platform? (Yes/No)

Q: Is interface reuse tracked and governed? (Yes/No)

Q: Are APIs monitored for usage and performance? (Yes/No)

E: API management platform screenshots

E: Usage analytics reports

E: Interface lifecycle documentation

Q: Are all interfaces standardized and reusable? (Yes/No)

Q: Are APIs dynamically adjusted based on usage or policy? (Yes/No)

Q: Is interface behavior monitored and optimized in real time? (Yes/No)

E: Dynamic API gateway configuration

E: Real-time interface health dashboards

E: Policy-driven API orchestration logs

System Messaging

Q: Is there any real-time messaging between systems? (Yes/No)

Q: Are alerts triggered manually? (Yes/No)

E: Messaging pilot documentation

E: Batch file transfer logs

E: Manual alerting procedures

Q: Are messaging patterns defined? (Yes/No)

Q: Are some messages exchanged between systems? (Yes/No)

Q: Are alerts traceable? (Yes/No)

E: Messaging pilot documentation

E: Sample message formats

E: Alert logs

Q: Are messages exchanged using standard formats? (Yes/No)

Q: Do messages trigger automated alerts? (Yes/No)

E: Messaging architecture diagrams

E: Message schemas (e.g., HL7 and FHIR)

E: Alert correlation rules

Q: Are critical messages exchanged in real time? (Yes/No)

Q: Are alerts integrated with monitoring tools? (Yes/No)

Q: Are messaging SLAs tracked? (Yes/No)

Q: Are messaging patterns reused across systems? (Yes/No)

E: Real-time alert dashboards

E: Messaging middleware configuration

E: Service Level Agreement (SLA) monitoring reports

Q: Are messages used to proactively address issues? (Yes/No)

Q: Are alerts predictive and self-resolving? (Yes/No)

Q: Is messaging fully event-driven? (Yes/No)

E: Real-time alert dashboards

E: Event-driven architecture documentation

E: Predictive alerting dashboards

E: Automated remediation logs

External Partner Integration

Q: Are partner integrations standardized? (Yes/No)

Q: Are integration endpoints managed centrally? (Yes/No)

E: Ad hoc partner connection logs

E: Lack of endpoint registry

E: Manual onboarding procedures

Q: Are integration patterns defined for partners? (Yes/No)

Q: Are endpoint management processes emerging? (Yes/No)

Q: Draft partner integration patterns

E: Endpoint inventory spreadsheet

E: Sample onboarding checklist

Q: Are new partner integrations following standard patterns? (Yes/No)

Q: Are endpoints registered and governed? (Yes/No)

Q: Are partner agreements documented? (Yes/No)

E: Partner integration playbook

E: Endpoint registration forms

E: Security and data-sharing agreements

Q: Are most partner integrations standardized and monitored? (Yes/No)

Q: Is endpoint management automated? (Yes/No)

Q: Are integration SLAs enforced? (Yes/No)

E: Automated endpoint management tool

E: Partner integration dashboards

E: SLA compliance reports

Q: Are partner integrations consolidated and reusable? (Yes/No)

Q: Is endpoint management integrated with ITSM? (Yes/No)

Q: Are partner connections monitored in real time? (Yes/No)

E: Shared services integration catalog

E: Real-time partner health dashboards

E: ITSM integration logs

Table 8 presents the Technical Architecture maturity criteria for Platform Services.

Table 8. Technical Architecture Maturity Criteria: Platform Services

Platform Services

Aspect

Level 1:Initial

Level 2:Developing

Level 3: Defined

Level 4: Managed

Level 5: Optimized

Applications are hosted in environments lacking platform standards or strategy.

Hosting uses dedicated servers or virtual machines with limited scaling capability.

There is very little automation of platform services.

Commonly used services such as document management workflow, rules management, and address validation, are deployed independently when they are needed.

Applications are hosted in environments using standards for platform capabilities.

Hosting primarily uses virtual servers.

The platform supports use of APIs and integration with Commercial Off-the-Shelf (COTS) tools and SaaS services.

Business rules and workflow capabilities are restricted to limited business functions.

Common services are used for a single module within a single vendor or state environment.

Platforms include Platform as a Service (PaaS) services such as databases, application containers, API security, and network integration.

The platform uses SaaS and leveraged COTS tools for business capabilities such as workflow, rules-based processing, and document management.

Common services are leveraged across modules but within the domain of a single vendor or state environment.

PaaS services are the primary hosting platform.

The platforms are designed and implemented using standards to limit complexity and to support automation.

Commonly used business and technical capabilities are implemented using SaaS and COTS tools.

Some tools are available across vendor and state hosting environments using APIs.

Platforms are based almost exclusively on PaaS services.

Platform standards include integration with operations management.

APIs are used to manage hosting including automated scaling.

Most services used across modules and environments are available from a common source although modules may use their own capabilities.

Platform Services Detail

Application Hosting

Applications are hosted on dedicated physical or virtual servers.

No platform standards.

Limited scalability and automation.

Hosting environments use virtual servers with some standardization.

SaaS and COTS tools are piloted.

APIs are used for limited integration.

Hosting platforms include PaaS services (e.g., databases, containers, and identity).

SaaS and COTS tools are integrated for business capabilities.

PaaS or SaaS is the primary hosting model.

Platforms are designed using standards to support automation, scalability, and cross-vendor integration.

Hosting is cloud native and policy driven.

APIs manage deployment, scaling, and monitoring.

Platforms are integrated with operations and support multi-tenant, cross-state reuse.

Business Rules and Workflow

Business rules and workflows are hardcoded within applications.

No reuse or externalization.

Rules and workflows are coded but follow emerging standards.

Some COTS tools for specific functions.

COTS or SaaS tools are used to manage rules and workflows.

Reuse is limited to specific modules or vendors.

Rules and workflows are externalized and reused across modules.

Tools support versioning, testing, and governance.

Rules and workflows are dynamically orchestrated across systems.

Business users can configure logic.

AI/ML supports optimization and exception handling.

Common Platform Functions

Common services are developed independently within each module.

No reuse or standardization.

Some services (e.g., document upload and address validation) are reused within a single module or vendor environment.

Shared services are available across multiple modules.

APIs and documentation support reuse.

Common services are standardized, governed, and integrated with platform operations.

Usage is monitored and optimized.

Services are composable, discoverable, and reused across states and vendors.

Service catalogs and self-service provisioning are available.

Platform Services Assessment / Evidence

Application Hosting

Q: Are applications hosted on dedicated servers? (Yes/No)

Q: Is there a platform strategy? (Yes/No)

E: Server inventory, unmanaged hosting screenshots

Q: Are virtual servers used consistently?  (Yes/No)

Q: Are SaaS tools piloted? (Yes/No)

E: Virtual server deployment plan, SaaS pilot documentation

Q: Are PaaS services used for hosting? (Yes/No)

Q: Are APIs integrated with hosting platforms? (Yes/No)

E: PaaS architecture diagrams, container usage logs

Q: Are platforms designed for automation and reuse? (Yes/No)

Q: Is cross-vendor integration supported? (Yes/No)

E: Platform standards, cross-vendor integration documentation

Q: Are platforms cloud native and policy driven? (Yes/No)

Q: Are services reused across states / vendors? (Yes/No)

E: API-driven deployment scripts, multi-tenant hosting logs

Business Rules and Workflow

Q: Are rules / workflows hard coded? (Yes/No)

Q: Is there any reuse? (Yes/No)

E: Application code with embedded rules, lack of external tools

Q: Are rules coded but follow standards?( (Yes/No)

Q: Are COTS tools used for specific functions? (Yes/No)

E: COTS tool pilot reports, rule documentation

Q: Are rules / workflows externalized? (Yes/No)

Q: Are they reused across modules?  (Yes/No)

E: SaaS rule engine screenshots, workflow configuration files

Q: Are rules versioned and governed? (Yes/No)

Q: Are they reused across vendors? (Yes/No)

E: Rule governance policy, versioning logs

Q: Can business users configure rules? (Yes/No)

Q: Are rules dynamically orchestrated? (Yes/No)

E: AI-optimized rule engine logs, business user configuration User Interface (UI)I

Common Platform Functions

Q: Are services developed independently by module? (Yes/No)

Q: Is there any reuse? (Yes/No)

E: Module-specific services (e.g., file upload), no shared catalog

Q: Are services reused within a module or vendor? (Yes/No)

Q: Is reuse documented? (Yes/No)

E: Internal service registry, single-module reuse examples

Q: Are services reused across multiple modules? (Yes/No)

Q: Are APIs documented and discoverable? (Yes/No)

E: API documentation for shared services, usage logs

Q: Are services standardized and monitored? (Yes/No)

Q: Is usage tracked across modules? (Yes/No)

E: Service governance policy, monitoring dashboards

Q: Are services composable and reused across states/vendors? (Yes/No)

Q: Is provisioning self-service? (Yes/No)

E: Service catalog, self-service provisioning portal

Table 9 presents the Technical Architecture maturity criteria for Application Architecture.

Table 9. Technical Architecture Maturity Criteria: Application Architecture

Application Architecture

Aspect

Level 1:Initial

Level 2:Developing

Level 3:Defined

Level 4:Managed

Level 5:Optimized

Medicaid systems are monolithic and tightly coupled, with limited ability to scale, replace, or integrate components.

User interfaces are inconsistent and inaccessible, and session management is unreliable or nonexistent.

There is no architectural governance or reuse.

Some modules or services are isolated and piloted, but integration and reuse are limited.

User interfaces begin to adopt responsive design and accessibility guidelines.

Session handling is basic and inconsistent across systems.

Architecture decisions are ad hoc.

Systems are modularized with standardized APIs and service contracts.

UI components are consistent and accessible, and session state is maintained across modules.

Architecture is documented and aligned with CMS interoperability standards.

Modular architecture is governed and reused across the enterprise.

Interfaces are mobile-first, role-based, and tested for accessibility.

Session and state are centrally managed and secure.

Architecture supports scalability, monitoring, and DevOps integration.

The architecture is fully composable, adaptive, and event driven.

Interfaces are personalized and continuously improved using analytics.

Session and state management is intelligent, context-aware, and resilient across devices and channels.

Architecture enables rapid innovation and seamless integration with external systems.

Application Architecture Detail

Modular Architecture

Applications are monolithic and tightly coupled.

Changes require full redeployment.

No service boundaries or reuse.

Some components are isolated or piloted as services.

Modularization is informal.

No formal service contracts or governance.

Core systems are decomposed into modules with standardized APIs and service contracts.

Modules can be deployed independently.

Modular architecture is governed by patterns and standards.

Shared services are reused across modules.

Modules are independently scalable and monitored.

Architecture is fully composable and event driven.

Modules are dynamically orchestrated and integrate seamlessly with external systems.

Architecture supports rapid innovation and plug-and-play capabilities.

User Interfaces

Interfaces are static and form based.

Design is inconsistent.

No accessibility compliance.

Some interfaces are redesigned for responsiveness.

Accessibility guidelines are drafted.

Basic usability testing is conducted.

Standardized UI components are used.

Interfaces comply with (Web Content Accessibility Guidelines (WCAG) 2.1 AA.

User feedback loops are established.

Design systems are reused across teams.

Interfaces are mobile first and role based.

Accessibility is tested automatically.

Interfaces are adaptive and personalized.

AI/ML supports User Experience (UX) optimization.

Continuous improvement is driven by analytics and user behavior.

Session and State Management

No session persistence.

Users lose progress on timeout.

No support for multi-tab or multi-device continuity.

Basic session timeout and login persistence.

Draft saving is available in some modules.

Session handling is inconsistent.

Session state is maintained across modules.

Drafts are recoverable.

Session expiration is policy driven.

Centralized session and state management are implemented.

Multi-device continuity is supported.

Session data is encrypted and monitored.

Session state is preserved across devices and sessions.

Context-aware restoration is enabled.

Session behavior adapts to user patterns and risk signals.

Application Architecture Assessment / Evidence

Modular Architecture

Q: Are applications monolithic and tightly coupled? (Yes/No)

Q: Is modularity absent? (Yes/No)

E: Legacy system diagrams, full redeployment logs

Q: Are some components isolated or piloted as services? (Yes/No)

Q: Are service boundaries informal? (Yes/No)

E: Pilot service documentation, partial modularization notes

Q: Are modules deployed independently with standardized APIs? (Yes/No)

Q: Are service contracts defined? (Yes/No)

E: API specifications, service contract templates, modular architecture diagrams

Q: Are modular patterns governed and reused? (Yes/No)

Q: Are modules independently scalable and monitored? (Yes/No)

E: Architecture governance policy, monitoring dashboards

Q: Is the architecture fully composable and event driven? (Yes/No)

Q: Are services dynamically orchestrated? (Yes/No)

E: Event-driven architecture diagrams, orchestration engine logs

User Interfaces

Q: Are interfaces static and inconsistent? (Yes/No)

Q: Is accessibility unsupported? (Yes/No)

E: Screenshots of legacy UIs, lack of WCAG documentation

Q: Are some interfaces responsive or redesigned? (Yes/No)

Q: Are accessibility guidelines drafted? (Yes/No)

E: Draft UI standards, usability test results

Q: Are standardized UI components used? (Yes/No)

Q: Is WCAG 2.1 AA compliance achieved? (Yes/No)

E: UI component library, accessibility audit reports

Q: Are design systems reused across teams? (Yes/No)

Q: Are interfaces mobile first and role based? (Yes/No)

E: Design system documentation, automated accessibility test logs

Q: Are interfaces adaptive and personalized? (Yes/No)

Q: Is UX continuously optimized using analytics? (Yes/No)

E: UX analytics dashboards, AI-driven personalization logs

Session and State Management

Q: Is session persistence absent? (Yes/No)

Q: Do users lose progress on timeout? (Yes/No)

E: Session timeout logs, lack of session recovery features

Q: Is basic session handling implemented? (Yes/No)

Q: Are drafts saved in some modules? (Yes/No)

E: Draft-saving feature screenshots, session timeout settings

Q: Is session state maintained across modules? (Yes/No)

Q: Are expiration policies defined? (Yes/No)

E: Session management policy, cross-module session logs

Q: Is session/state management centralized? (Yes/No)

Q: Is multi-device continuity supported? (Yes/No)

E: Session encryption logs, centralized session service documentation

Q: Is session behavior context-aware and adaptive? (Yes/No)

Q: Is session state preserved across devices and sessions? (Yes/No)

E: AI-driven session management logs, cross-device continuity test results

Table 10 presents the Technical Architecture maturity criteria for Security and Identity.

Table 10. Technical Architecture Maturity Criteria: Security and Identity

Security and Identity

Aspect

Level 1:Initial

Level 2:Developing

Level 3:Defined

Level 4:Managed

Level 5:Optimized

Security controls are dispersed without oversight.

Multiple identity management systems may be in place.

Minimal capabilities to manage and use member consent.

Lack of visibility to security protections across the enterprise.

Limited and siloed monitoring.

Some consolidation of identity management systems.

Member consent management is in place.

Consent management is in place, but some processes such as revoking consent are manual.

Standards exist for security protection and monitoring, but tare not fully implemented.

Some applications use standalone identity management systems.

Consent management in place.

Security protections are defined and implemented for most systems.

Security Information and Event Management (SIEM) tool used on most systems.

Single identity management system used by all applications.

Consent management is in place.

Security controls are standardized, implemented, and meet state and federal regulations. Internal audits are used to validate compliance.

A single SIEM tool provides view of enterprise security events.

The architecture is fully composable, adaptive, and event driven.

Interfaces are personalized and continuously improved using analytics.

Session and state management is intelligent, context-aware, and resilient across devices and channels.

Architecture enables rapid innovation and seamless integration with external systems.

Security and Identity Detail

Identity and Access Services

Multiple identity systems are in place with inconsistent standards.

Access is manually provisioned.

No centralized authentication or audit trail.

Identity systems are partially consolidated.

Some access policies are defined.

Authentication is standardized for new systems.

A centralized identity and access management (IAM) system is used for most applications.

Role-based access control (RBAC) is implemented.

All applications use a single IAM system.

Multi-factor authentication (MFA) is enforced.

Access provisioning is automated and auditable.

Identity is federated across systems and partners.

Adaptive access controls respond to risk signals.

IAM is integrated with DevSecOps and zero trust architecture.

Consent Management

Consent is captured manually or inconsistently.

No centralized tracking or enforcement.

Consent policies are defined.

Some systems capture and store consent electronically.

Revocation is manual.

Consent is captured and enforced across systems.

Electronic consent records are auditable.

Revocation is supported through standard workflows.

Consent is managed through centralized services.

APIs enforce consent dynamically.

Audit logs are integrated with compliance reporting.

Consent is context aware and member controlled.

Real-time enforcement and revocation are supported.

Consent services are interoperable across agencies and partners.

System and Data Protection

Security controls are inconsistent and manually applied.

No encryption or secure configuration standards.

Security standards are defined and partially implemented.

Some systems use encryption and secure configurations.

Security controls are implemented across systems.

Encryption is enforced for data at rest and in transit. Internal audits are conducted.

Security controls are continuously monitored and updated.

Automated compliance checks are in place.

Incident response plans are tested.

Security controls exceed regulatory requirements.

AI/ML supports threat detection and response.

Continuous compliance is achieved through automated validation.

Security Monitoring

Security event monitoring is siloed and inconsistent.

No centralized visibility or alerting.

Monitoring standards are defined.

Some systems use commercial or open-source tools.

Alerts are manually reviewed.

A SIEM tool is used across most systems.

Alerts are correlated and logged.

A centralized SIEM provides enterprise-wide visibility.

Automated alerting and response workflows are in place.

SIEM is integrated with AI/ML for predictive threat detection.

Automated remediation is triggered by risk signals.

Monitoring supports real-time compliance and audit readiness.

Security and Identity Assessment / Evidence

Identity and Access Services

Q: Are identity systems fragmented and unmanaged? (Yes/No)

Q: Is access manually provisioned? (Yes/No)

E: Screenshots of legacy IAM tools, manual access logs

Q: Are identity systems partially consolidated?(Yes/No)

Q: Are access policies defined for new systems? (Yes/No)

E: IAM policy drafts, partial integration diagrams

Q: Is a centralized IAM system used for most applications? (Yes/No)

Q: Is RBAC implemented? Yes/No)

E: IAM architecture diagrams, RBAC configuration files

Q: Is MFA enforced across all applications? (Yes/No)

Q: Is access provisioning automated and auditable? (Yes/No)

E: MFA logs, automated access request workflows

Q: Is identity federated across systems and partners? (Yes/No)

Q: Are adaptive access controls in place? (Yes/No)

E: Federation trust configurations, risk-based access control logs

Consent Management

Q: Is consent captured manually or inconsistently? (Yes/No)

Q: Is there centralized tracking? (Yes/No)

E: Paper forms, inconsistent consent logs

Q: Are consent policies defined? (Yes/No)

Q: Are electronic consents partially implemented? (Yes/No)

E: Draft consent policy, screenshots of consent capture UI

Q: Is consent enforced across systems? (Yes/No)

Q: Is revocation supported through standard workflows? (Yes/No)

E: Consent audit logs, revocation request workflows

Q: Is consent managed through centralized services? (Yes/No)

Q: Are APIs used to enforce consent dynamically? (Yes/No)

E: Consent service API specifications, audit trail dashboards

Q: Is consent context aware and member controlled? (Yes/No)

Q: Is real-time revocation supported? (Yes/No)

E: Member portal screenshots, real-time consent enforcement logs

System and Data Protection

Q: Are security controls inconsistent and manually applied? (Yes/No)

Q: Is encryption missing or partial? (Yes/No)

E: Unencrypted data logs, ad hoc firewall rules

Q: Are security standards defined but not fully implemented? (Yes/No)

Q: Are some systems encrypted? (Yes/No)

E: Draft security standards, partial encryption reports

Q: Are security controls implemented across systems? (Yes/No)

Q: Are internal audits conducted? (Yes/No)

E: Audit reports, encryption policy enforcement logs

Q: Are controls continuously monitored and updated? (Yes/No)

Q: Are incident response plans tested? (Yes/No)

E: SIEM alerts, DR/Incident Response (IR) test results

Q: Do controls exceed regulatory requirements? (Yes/No)

Q: Is AI/ML used for threat detection? (Yes/No)

E: Threat intelligence dashboards, automated remediation logs

Security Monitoring

Q: Is monitoring siloed and inconsistent? (Yes/No)

Q: Is there centralized visibility? (Yes/No)

E: System-specific logs, lack of SIEM integration

Q: Are monitoring standards defined? (Yes/No)

Q: Are alerts manually reviewed? (Yes/No)

E: Monitoring policy drafts, alert review logs

Q: Is a SIEM tool used across most systems? (Yes/No)

Q: Are alerts correlated and logged? (Yes/No)

E: SIEM dashboards, alert correlation rules

Q: Is a centralized SIEM used enterprise wide? (Yes/No)

Q: Are automated alert responses in place? (Yes/No)

E: SIEM integration diagrams, automated response workflows

Q: Is AI/ML used for predictive threat detection? (Yes/No)

Q: Is remediation automated? (Yes/No)

E: Predictive analytics dashboards, auto-remediation logs

Table 11 presents the Technical Architecture maturity criteria for Operations and Maintenance.

Table 11. Technical Architecture Maturity Criteria: Operations and Maintenance

Operations and Monitoring

Aspect

Level 1: Initial

Level 2:Developing

Level 3:Defined

Level 4:Managed

Level 5:Optimized

System monitoring implemented on a system-by-system basis.

Monitoring identifies problems after they happen.

Tools for operations are in place, but manual processes are still required to operate system.

Guidelines for system monitoring are in place.

Some commercial or open-source tools for monitoring.

Standard processes are in place for operations.

System updates and patching.

Most systems use the same tools for monitoring and alerts.

Some automation is in place to respond to problems.

Dashboards present information on system problems.

System operations processes use automation and are developing alignment with standards such as Information Technology Infrastructure Library (ITIL).

Monitoring and alerting are automated and integrated across systems.

Proactive monitoring is implemented for both platforms and applications.

Automation is used to respond to problems.

Dashboards present information about the system across multiple modules.

Systems operations processes are extensively automated and aligned with system management standards such as ITIL.

Monitoring is controlled through configuration.

Wide use of proactive monitoring and automated problem prevention and response.

Dashboards present information about the current system health and expectations for future activities operation across the MES.

Operations and Monitoring Detail

System Monitoring

Monitoring is implemented independently per system.

Alerts are reactive and manually reviewed.

No enterprise-wide visibility.

Guidelines for monitoring are defined.

Some systems use commercial or open-source tools.

Alerts are partially automated.

A common set of tools is used across most systems.

Dashboards present system health.

Alerts are integrated and monitored centrally.

Monitoring is automated and integrated across platforms and applications.

Proactive alerting and root cause analysis are in place.

Dashboards provide real-time visibility across modules.

Monitoring is configuration driven and policy based.

AI/ML supports predictive alerts and automated remediation.

Dashboards forecast system health and future risks across the enterprise.

System Operations

Operations are manual and system specific.

Batch jobs and file transfers are managed independently.

Updates are delayed and cause downtime.

Routine tasks (e.g., file transfers and job scheduling) are partially automated.

Standard procedures exist for updates, but execution is manual.

Many operations are automated.

Patching is scheduled and tested.

Updates follow defined procedures and avoid unplanned downtime.

Operations are extensively automated and aligned with ITSM standards (e.g., ITIL).

Updates are implemented without downtime.

Operations are fully automated and policy driven.

Updates are continuous and risk managed.

AI/ML supports anomaly detection and self-healing.

Operations exceed ITSM benchmarks.

Operations and Monitoring / Evidence

System Monitoring

Q: Is monitoring siloed and reactive? (Yes/No)

Q: Is there centralized visibility? (Yes/No)

E: System-specific logs, manual alerting procedures

Q: Are monitoring standards defined? (Yes/No)

Q: Are commercial / open-source tools used on some systems? (Yes/No)

E: Monitoring policy drafts, tool deployment screenshots

Q: Are common tools used across most systems? (Yes/No)

Q: Are alerts integrated and monitored centrally? (Yes/No)

E: Unified dashboards, alert correlation rules

Q: Is monitoring automated and proactive? (Yes/No)

Q: Are dashboards used for real-time visibility? (Yes/No)

E: Real-time monitoring dashboards, root cause analysis logs

Q: Is monitoring configuration-driven and predictive? (Yes/No)

Q: Is AI/ML used for alerting and remediation? (Yes/No)

E: Predictive analytics dashboards, automated remediation logs

System Operations

Q: Are operations manual and system specific? (Yes/No)

Q: Are updates delayed or disruptive? (Yes/No)

E: Manual job logs, update failure reports

Q: Are routine tasks partially automated? (Yes/No)

Q: Are update procedures standardized but manual? (Yes/No)

E: File transfer scripts, patching SOPs

Q: Are many operations automated? (Yes/No)

Q: Are updates scheduled and tested? (Yes/No)

E: Automation scripts, patching schedules, downtime logs

Q: Are operations aligned with ITSM standards? (Yes/No)

Q: Are updates implemented without downtime? (Yes/No)

E: ITIL-aligned process documents, zero-downtime deployment logs

Q: Are operations fully automated and policy-driven? (Yes/No)

Q: Is AI/ML used for anomaly detection and self-healing? (Yes/No)

E: AI-driven ops dashboards, automated patching and rollback logs

Table 12 presents the Technical Architecture maturity criteria for Development and Release.

Table 12. Technical Architecture Maturity Criteria: Development and Release

Development and Release

Aspect

Level 1:Initial

Level 2:Developing

Level 3:Defined

Level 4:Managed

Level 5:Optimized

Development and release processes are manual, inconsistent, and reactive.

Code changes are not version controlled, testing is ad hoc, and security is addressed after deployment.

No formal governance or automation.

Basic tools and practices are introduced, such as version control and environment separation.

Some testing and security practices are piloted, but processes remain siloed and inconsistently applied.

Releases are still largely manual.

Development and release processes are standardized and documented.

Automated testing and IAC are introduced.

Security and compliance checks are integrated into the development lifecycle.

Releases follow defined gates.

Standard practices and approaches (e.g., CI/CD pipelines) are used to automate builds, tests, and deployments.

Code and infrastructure changes are peer reviewed and traceable.

Security gates and rollback mechanisms are embedded.

Release metrics are tracked and used for improvement.

Development and release are continuous, adaptive, and policy driven.

AI/ML supports test optimization, anomaly detection, and predictive compliance.

The entire pipeline is monitored in real time, enabling rapid, secure, and resilient delivery of changes.

Development and Release Detail

Code and Configuration Management

Code and infrastructure settings are managed manually.

No version control.

Environments are inconsistently configured.

Basic version control (e.g., Git) is introduced.

Teams begin using scripts for repeatable configurations.

Infrastructure settings are partially documented.

Version control is standardized across teams.

IAC is adopted for provisioning.

Code and configuration changes are peer reviewed and traceable.

IaC is integrated into CI/CD pipelines.

Code and infrastructure changes are governed by policy and auditable.

Repositories are version controlled and monitored.

Code and infrastructure are fully automated and policy driven.

AI/ML supports anomaly detection and predictive configuration.

Compliance is enforced continuously.

Testing and Release

Testing is manual and inconsistent.

Releases are infrequent and error prone.

No formal environments or release process.

Basic test automation is introduced.

Dev/Test/Prod environments are established.

Releases follow informal checklists.

Automated testing is integrated into CI/CD pipelines.

Release gates and rollback procedures are defined.

Releases are traceable and documented.

CI/CD pipelines are standardized and monitored.

Releases are automated, peer reviewed, and include rollback and canary deployments.

Metrics (e.g., deployment frequency and failure rate) are tracked.

Releases are continuous and adaptive.

AI/ML optimizes test coverage and prioritization.

Pipelines are self-healing and policy driven.

Release health is monitored in real time.

Security and Compliance

Security is addressed post-deployment.

No secure coding practices or compliance checks.

Security policies are drafted.

Static code analysis tools are piloted.

Manual compliance reviews occur before release.

Secure coding standards are enforced.

Security scans (Static Application Security Testing [SAST] / Dynamic Application Security Testing [DAST]) are integrated into pipelines.

Compliance checklists are standardized.

DevSecOps practices are adopted.

Security gates block non-compliant code.

Compliance evidence is automatically collected and stored.

Continuous compliance is achieved.

AI/ML detects security anomalies and enforces policy.

Compliance dashboards provide real-time visibility across the enterprise.

Development and Release Assessment / Evidence

Code and Configuration Management

Q: Is code managed manually without version control? (Yes/No)

Q: Are infrastructure settings inconsistently applied? (Yes/No)

E: Shared drive code folders, manual configuration logs

Q: Is version control used by some teams? (Yes/No)

Q: Are scripts used for repeatable configurations? (Yes/No)

E: Git repo screenshots, provisioning scripts

Q: Is version control standardized across teams? (Yes/No)

Q: Is IAC adopted? (Yes/No)

E: IaC templates, peer review logs

Q: Is IaC integrated into CI/CD pipelines? (Yes/No)

Q: Are code / configuration changes governed and auditable? (Yes/No)

E: CI/CD pipeline configurations, change audit logs

Q: Are code and infrastructure fully automated and policy-driven? (Yes/No)

Q :Is AI/ML used for anomaly detection? (Yes/No)

E: AI-based configuration monitoring, policy enforcement dashboards

Testing and Release

Q: Is testing manual and inconsistent? (Yes/No)

Q:Are releases infrequent and error prone? (Yes/No)

E: Manual test scripts, release failure reports

Q: Is basic test automation introduced? (Yes/No)

Q: Are Dev/Test/Prod environments established? (Yes/No)

E: Test automation logs, environment setup documents

Q: Are automated tests integrated into CI/CD? (Yes/No)

Q: Are release gates and rollback procedures defined? (Yes/No)

E: CI/CD pipeline configurations, rollback playbooks

Q: Are releases automated and peer reviewed? (Yes/No)

Q: Are metrics like deployment frequency tracked? (Yes/No)

E: Deployment dashboards, release audit logs

Q: Are releases continuous and adaptive? (Yes/No)

Q: Is AI/ML used for test optimization and risk detection? (Yes/No)

E: Predictive release analytics, self-healing pipeline logs

Security and Compliance

Q: Is security addressed post-deployment? (Yes/No)

Q: Are secure coding practices absent? (Yes/No)

E: Post-release vulnerability reports, lack of security checklists

Q: Are security policies drafted? (Yes/No)

Q: Analysis tools piloted? (Yes/No)

E: SAST tool outputs, draft security policies

Q: Are secure coding standards enforced? (Yes/No)

Q: Are security scans integrated into pipelines? (Yes/No)

E: SAST/DAST logs, compliance checklists

Q: Are DevSecOps practices adopted? (Yes/No)

Q: Is compliance evidence automatically collected? (Yes/No)

E: Security gate configurations, compliance audit logs

Q: Is continuous compliance achieved? (Yes/No)

Q: Is AI/ML used for anomaly detection and enforcement? (Yes/No)

E: Real-time compliance dashboards, AI-driven security alerts

Using Personas To Describe Maturity Levels

MITA 4.0 adds personas to explain maturity levels and make abstract concepts more relatable, understandable, and actionable. Personas are fictional, representative profiles of an organization that embody specific characteristics and behaviors. MITA 4.0 adopted personas for explaining maturity levels to:

Improve Communication – Personas make discussions about maturity levels more engaging and accessible, especially for non-technical audiences. Rather than diving into technical details, personas tell a story about the organization’s journey, making it easier for everyone to understand and participate in the conversation.

Support Planning and Decision-Making – Personas help stakeholders identify where they currently stand and where they want to go. By aligning their organization with a persona at a specific maturity level, stakeholders can better plan the necessary steps to move to the next level and prioritize improvements based on the persona's challenges and goals.

Personas By turning abstract concepts into relatable stories, personas bring maturity levels to life. They help stakeholders understand where they are, where they want to go, and the necessary steps to get there, all while keeping the organization’s focus on real-world needs and outcomes.

Figure 2. MITA 4.0 Maturity Scale with Personas

Appendix A Maturity Profile Standard Template

The SMA will complete this template for each Capability Domain assessed.

Figure 3. Maturity Profile Standard Template